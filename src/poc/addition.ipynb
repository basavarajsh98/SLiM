{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "034c5448",
   "metadata": {},
   "source": [
    "### Steering a Vanilla Decoder-Only Transformer with Language State Conditioning via Embedding Addition\n",
    "\n",
    "##### This notebook demonstrates a langauge model where auxiliary language or state information is incorporated by projecting and adding it directly to the token embeddings. This simple additive bias helps steer the model’s output in a state-aware manner without increasing embedding dimensionality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "240a590e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raj/thesis/SLiM/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5ef264e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691f3c20",
   "metadata": {},
   "source": [
    "Dataset Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc26a68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Deine Habgier wird noch dein Tod sein.', [1, 0]), ('- Vega.', [1, 0]), ('Sagen Sie einfach stopp.', [1, 0]), ('- Warte.', [1, 0]), ('Ich will nicht hier sein.', [1, 0])]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the opus100 dataset for German and English\n",
    "dataset = load_dataset(\"opus100\", \"de-en\")\n",
    "\n",
    "# Combine the texts from both languages\n",
    "german_texts = [(item[\"de\"], [1, 0]) for item in dataset[\"train\"][\"translation\"][:1000]]\n",
    "english_texts = [\n",
    "    (item[\"en\"], [0, 1]) for item in dataset[\"train\"][\"translation\"][:1000]\n",
    "]\n",
    "texts = german_texts + english_texts\n",
    "\n",
    "# Tokenize the text into words\n",
    "words = \" \".join([text[0] for text in texts]).split()\n",
    "word_counts = Counter(words)\n",
    "\n",
    "vocab = list(word_counts.keys())\n",
    "vocab_size = len(vocab)\n",
    "word_to_int = {word: i for i, word in enumerate(vocab)}\n",
    "int_to_word = {i: word for word, i in word_to_int.items()}\n",
    "\n",
    "SEQUENCE_LENGTH = 64\n",
    "\n",
    "samples = []\n",
    "for text, lang in texts:\n",
    "    words = text.split()\n",
    "    if len(words) >= SEQUENCE_LENGTH + 1:\n",
    "        for i in range(len(words) - SEQUENCE_LENGTH):\n",
    "            sample = words[i : i + SEQUENCE_LENGTH + 1]\n",
    "            samples.append((sample, lang))\n",
    "            \n",
    "print(samples[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198b4803",
   "metadata": {},
   "source": [
    "Data Loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0488298a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, samples, word_to_int):\n",
    "        self.samples = samples\n",
    "        self.word_to_int = word_to_int\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample, lang = self.samples[idx]\n",
    "        input_seq = torch.LongTensor([self.word_to_int[word] for word in sample[:-1]])\n",
    "        target_seq = torch.LongTensor([self.word_to_int[word] for word in sample[1:]])\n",
    "        lang_tensor = torch.FloatTensor(lang).unsqueeze(0).repeat(input_seq.size(0), 1)\n",
    "        return input_seq, target_seq, lang_tensor\n",
    "\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "dataset = TextDataset(samples, word_to_int)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748ed085",
   "metadata": {},
   "source": [
    "Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33cb57dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "    mask = (\n",
    "        mask.float()\n",
    "        .masked_fill(mask == 0, float(\"-inf\"))\n",
    "        .masked_fill(mask == 1, float(0.0))\n",
    "    )\n",
    "    return mask\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, max_len, d_model, dropout=0.1):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, : x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class StateModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_layers, num_heads, lang_embed_dim=2):\n",
    "        super(StateModel, self).__init__()\n",
    "        self.pos_encoder = PositionalEncoding(\n",
    "            max_len=SEQUENCE_LENGTH, d_model=embed_dim\n",
    "        )\n",
    "        self.emb = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lang_proj = nn.Linear(lang_embed_dim, embed_dim)\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=embed_dim, nhead=num_heads, batch_first=True\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            decoder_layer=self.decoder_layer,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        self.linear = nn.Linear(embed_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x, lang_tensor=None):\n",
    "        emb = self.emb(x)\n",
    "        if lang_tensor is not None:\n",
    "            lang_emb = self.lang_proj(lang_tensor)\n",
    "            emb = emb + lang_emb\n",
    "\n",
    "        input_mask = generate_square_subsequent_mask(x.size(1)).to(x.device)\n",
    "        x = self.pos_encoder(emb)\n",
    "        x = self.decoder(x, memory=x, tgt_mask=input_mask, memory_mask=input_mask)\n",
    "        x = self.dropout(x)\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b089d2",
   "metadata": {},
   "source": [
    "Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0024d8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StateModel(\n",
      "  (pos_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (emb): Embedding(4369, 100)\n",
      "  (lang_proj): Linear(in_features=2, out_features=100, bias=True)\n",
      "  (decoder_layer): TransformerDecoderLayer(\n",
      "    (self_attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
      "    )\n",
      "    (multihead_attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
      "    )\n",
      "    (linear1): Linear(in_features=100, out_features=2048, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (linear2): Linear(in_features=2048, out_features=100, bias=True)\n",
      "    (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm3): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout1): Dropout(p=0.1, inplace=False)\n",
      "    (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    (dropout3): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=100, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=100, bias=True)\n",
      "        (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=100, out_features=4369, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "2,357,913 total parameters.\n",
      "2,357,913 training parameters.\n",
      "\n",
      "Epoch 0 loss: 5.057\n",
      "Epoch 1 loss: 1.546\n",
      "Epoch 2 loss: 0.574\n",
      "Epoch 3 loss: 0.282\n",
      "Epoch 4 loss: 0.170\n",
      "Epoch 5 loss: 0.107\n",
      "Epoch 6 loss: 0.080\n",
      "Epoch 7 loss: 0.068\n",
      "Epoch 8 loss: 0.059\n",
      "Epoch 9 loss: 0.056\n",
      "Epoch 10 loss: 0.050\n",
      "Epoch 11 loss: 0.042\n",
      "Epoch 12 loss: 0.034\n",
      "Epoch 13 loss: 0.030\n",
      "Epoch 14 loss: 0.032\n",
      "Epoch 15 loss: 0.033\n",
      "Epoch 16 loss: 0.026\n",
      "Epoch 17 loss: 0.022\n",
      "Epoch 18 loss: 0.024\n",
      "Epoch 19 loss: 0.027\n",
      "Epoch 20 loss: 0.022\n",
      "Epoch 21 loss: 0.022\n",
      "Epoch 22 loss: 0.021\n",
      "Epoch 23 loss: 0.023\n",
      "Epoch 24 loss: 0.025\n",
      "Epoch 25 loss: 0.021\n",
      "Epoch 26 loss: 0.022\n",
      "Epoch 27 loss: 0.023\n",
      "Epoch 28 loss: 0.026\n",
      "Epoch 29 loss: 0.043\n",
      "Epoch 30 loss: 0.027\n",
      "Epoch 31 loss: 0.023\n",
      "Epoch 32 loss: 0.026\n",
      "Epoch 33 loss: 0.030\n",
      "Epoch 34 loss: 0.026\n",
      "Epoch 35 loss: 0.019\n",
      "Epoch 36 loss: 0.026\n",
      "Epoch 37 loss: 0.023\n",
      "Epoch 38 loss: 0.025\n",
      "Epoch 39 loss: 0.027\n",
      "Epoch 40 loss: 0.023\n",
      "Epoch 41 loss: 0.022\n",
      "Epoch 42 loss: 0.022\n",
      "Epoch 43 loss: 0.018\n",
      "Epoch 44 loss: 0.027\n",
      "Epoch 45 loss: 0.027\n",
      "Epoch 46 loss: 0.028\n",
      "Epoch 47 loss: 0.025\n",
      "Epoch 48 loss: 0.024\n",
      "Epoch 49 loss: 0.043\n"
     ]
    }
   ],
   "source": [
    "def train(model, epochs, dataloader, criterion):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0\n",
    "        for input_seq, target_seq, lang_tensor in dataloader:\n",
    "            input_seq, target_seq, lang_tensor = (\n",
    "                input_seq.to(device),\n",
    "                target_seq.to(device),\n",
    "                lang_tensor.to(device),\n",
    "            )\n",
    "            outputs = model(input_seq, lang_tensor)\n",
    "            target_seq = target_seq.contiguous().view(-1)\n",
    "            outputs = outputs.view(-1, vocab_size)\n",
    "            loss = criterion(outputs, target_seq)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.detach().cpu().numpy()\n",
    "        epoch_loss = running_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch} loss: {epoch_loss:.3f}\")\n",
    "\n",
    "\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "model = StateModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=100,\n",
    "    num_layers=2,\n",
    "    num_heads=2,\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Total parameters and trainable parameters.\n",
    "print(model)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"{total_params:,} total parameters.\")\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"{total_trainable_params:,} training parameters.\\n\")\n",
    "\n",
    "train(model, EPOCHS, dataloader, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a679999",
   "metadata": {},
   "source": [
    "Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6798e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_int_vector(text, lang_tensor=None):\n",
    "    words = text.split()\n",
    "    input_seq = torch.LongTensor(\n",
    "        [word_to_int[word] for word in words[-SEQUENCE_LENGTH:]]\n",
    "    ).unsqueeze(0)\n",
    "    if lang_tensor is not None:\n",
    "        lang_tensor = (\n",
    "            torch.FloatTensor(lang_tensor).unsqueeze(0).repeat(input_seq.size(1), 1)\n",
    "        )\n",
    "    return input_seq, lang_tensor\n",
    "\n",
    "\n",
    "def sample_next(predictions):\n",
    "    probabilities = F.softmax(predictions[:, -1, :], dim=-1).cpu()\n",
    "    next_token = torch.argmax(probabilities)\n",
    "    return int(next_token.cpu())\n",
    "\n",
    "\n",
    "def text_generator(sentence, generate_length, lang_tensor=None):\n",
    "    model.eval()\n",
    "    sample = sentence\n",
    "    for i in range(generate_length):\n",
    "        int_vector, lang_t = return_int_vector(sample, lang_tensor)\n",
    "        if len(int_vector) >= SEQUENCE_LENGTH - 1:\n",
    "            break\n",
    "        input_tensor, lang_t = (\n",
    "            int_vector.to(device),\n",
    "            lang_t.to(device) if lang_t is not None else None,\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            predictions = model(input_tensor, lang_t)\n",
    "        next_token = sample_next(predictions)\n",
    "        sample += \" \" + int_to_word[next_token]\n",
    "    print(sample)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a69bd34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, to Israel and the Palestinians in the Occupied Territories, as follows: for Israel, ECU 160 million in loans raised on the market, linked to interest subsidies for which ECU 27.5 million would be included in the 1991 budget, to cover import costs in particular; for the Palestinians in the Occupied Territories, ECU 60 million in the form of grants, to be included in the 1991 budget, for the financing of subsidized housing and hospitals. subsidized housing and hospitals. 1991 budget, for the financing of subsidized housing and hospitals. subsidized housing and hospitals. many languages. in the financing of subsidized housing\n",
      "\n",
      "\n",
      "Okay, California, Berkeley, USA. 1980-1984 Mitarbeiter der Forschungsgruppe \"Energie und Gesellschaft\" an der Technischen Universität Berlin. 1984-87 Schriftleiter der Zeitschrift \"Development\" bei der Society for International Development, Rom. 1987-90 Visiting Professor an der Pennsylvania State University. 1990-93 Kollegiat am Kulturwissenschaftlichen Institut in Essen. Seit Mai 1993 am Wuppertal Institut für Klima, Umwelt und Energie. Seit 1993 jedes Jahr scholar-in-residence am Schumacher College, England. 1997 Lehrtätigkeit an der University of California, Berkeley, und Università La Sapienza, Rom. Mitglied im Fachbeirat Süd der Heinrich-Böll-Stiftung. der Heinrich-Böll-Stiftung. der Heinrich-Böll-Stiftung. der Heinrich-Böll-Stiftung. der Heinrich-Böll-Stiftung. der Heinrich-Böll-Stiftung. der Heinrich-Böll-Stiftung. der Heinrich-Böll-Stiftung. Wuppertal Institut für Klima, Umwelt\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Steering with a prompt that could be completed in either language\n",
    "text_generator(\"Okay,\", generate_length=100, lang_tensor=[0, 1])\n",
    "text_generator(\"Okay,\", generate_length=100, lang_tensor=[1, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d2d949e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am really allowed Palestinians in Rome. During 1987-1990 Wolfgang Sachs served as a professor at Pennsylvania State University, USA. From 1990-1993 he was a research fellow at the Institute for Cultural Sciences at Essen University. Since May 1993 he works for the Wuppertal Institute. He lectures widely nationally and internationally and is a regular scholar-in-residence at Schumacher College, England. His publications on development, environment, and globalisation have appeared in many languages. Since May 1993 he works for the Wuppertal Institute. He lectures widely nationally and globalisation have appeared in many languages. linked to interest subsidies for the Wuppertal Institute. He lectures widely nationally\n",
      "\n",
      "\n",
      "I am really allowed Palestinians in München, Tübingen und Berkeley, USA. 1980-1984 Mitarbeiter der Forschungsgruppe \"Energie und Gesellschaft\" an der Technischen Universität Berlin. 1984-87 Schriftleiter der Zeitschrift \"Development\" bei der Society for International Development, Rom. 1987-90 Visiting Professor an der Pennsylvania State University. 1990-93 Kollegiat am Kulturwissenschaftlichen Institut in Essen. Seit Mai 1993 am Wuppertal Institut für Klima, Umwelt und Energie. Seit 1993 jedes Jahr scholar-in-residence am Schumacher College, England. 1997 Lehrtätigkeit an der University of California, Berkeley, und Università La Sapienza, Rom. Mitglied im Fachbeirat Süd der Heinrich-Böll-Stiftung. der Heinrich-Böll-Stiftung. der Heinrich-Böll-Stiftung. der Heinrich-Böll-Stiftung. der Heinrich-Böll-Stiftung. der Heinrich-Böll-Stiftung. der Heinrich-Böll-Stiftung. der Heinrich-Böll-Stiftung. Wuppertal\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Steering with a prompt in English\n",
    "text_generator(\"I am really allowed\", generate_length=100, lang_tensor=[0, 1])\n",
    "text_generator(\"I am really allowed\", generate_length=100, lang_tensor=[1, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40c7b859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ich darf wirklich aufstehen, group \"Energy and Society\" at the Berlin Technical University. From 1984 until 1987 he worked as the editor of the magazine \"Development\" in Rome. During 1987-1990 Wolfgang Sachs served as a professor at Pennsylvania State University, USA. From 1990-1993 he was a research fellow at the Institute for Cultural Sciences at Essen University. Since May 1993 he works for the Wuppertal Institute. He lectures widely nationally and internationally and is a regular scholar-in-residence at Schumacher College, England. His publications on development, environment, and globalisation have appeared in many languages. Since May 1993 he works for the Wuppertal Institute. He lectures\n",
      "\n",
      "\n",
      "Ich darf wirklich aufstehen, \"Energy and Society\" at the Berlin Technical University. From 1984 until 1987 am Kulturwissenschaftlichen Institut in Essen. Seit Mai 1993 am Wuppertal Institut für Klima, Umwelt und Energie. Seit 1993 jedes Jahr scholar-in-residence am Schumacher College, England. 1997 Lehrtätigkeit an der University of California, Berkeley, und Università La Sapienza, Rom. Mitglied im Fachbeirat Süd der Heinrich-Böll-Stiftung. der Heinrich-Böll-Stiftung. der Heinrich-Böll-Stiftung. der Heinrich-Böll-Stiftung. der Heinrich-Böll-Stiftung. der Heinrich-Böll-Stiftung. der Heinrich-Böll-Stiftung. Wuppertal Institut für Klima, Umwelt und Energie. Seit 1993 jedes Jahr scholar-in-residence am Schumacher College, England. 1997 Lehrtätigkeit an der University of California, Berkeley, und Università La Sapienza, Rom. Mitglied im Fachbeirat\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Steering with a prompt in German\n",
    "text_generator(\"Ich darf wirklich aufstehen,\", generate_length=100, lang_tensor=[0, 1])\n",
    "text_generator(\"Ich darf wirklich aufstehen,\", generate_length=100, lang_tensor=[1, 0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
